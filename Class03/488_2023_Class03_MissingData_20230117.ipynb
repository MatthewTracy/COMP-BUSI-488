{"cells":[{"cell_type":"markdown","metadata":{"id":"y5hrTVC4OFK6"},"source":["# **BUSI 488 / COMP 488 Data Science in the Business World**\n","## *Spring 2023*  \n","Daniel M. Ringel  \n","Kenan-Flagler Business School  \n","*The University of North Carolina at Chapel Hill*  \n","dmr@unc.edu  \n","\n","---\n","## Missing Data Examples for Class 03\n","*January 17th, 2023*  \n","Version 1.2  \n","\n","\n","----"]},{"cell_type":"markdown","metadata":{"id":"WnflRdJ-W4YC"},"source":["# 0. Mount your drive where the dataset is located\n","\n","1. Run Cell below\n","2. Click on link ***Go to this URL in a browser:***\n","3. Login to your Google Drive in the new browser tab and allow access\n","4. OPTIONAL: Copy the generated authorization code (if you get one)\n","5. OPTIONAL: Paste the authorization code in the provided field ***Enter your authorization code:*** (if it is shown - not always the case)\n","6. Hit ENTER\n","7. You might also be promted to verify access through a code via text message"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"fkVZBvP2G03F"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"orCvDGQbG03G"},"source":["Let's navigate to the folder on your Google Drive that you uploaded the files for this notebook to:  \n","- \"MoviesOnStreamingPlatforms_updated.csv\"\n","- \"complaints-credit-debit-cards+fees-interest-2019-2020.csv\"  \n","\n","1. %cd is a command that means \"change directory\". In other words: go to the following folder\n","2. following %cd, you need to provide the \"path\" (i.e., hierachie of folders) to the folder where your data file(s) are.   \n","  \n","**Warning:** Your path does not include your file! Only the folder it is contained in!\n","\n","3. keep the first part /content/gdrive/MyDrive/ the same as in the code cell below\n","4. change the part /Teaching/2023/488/CoLab_Tutorial to your own folder structure (i.e., the folders you created on your google drive)\n","   - for example, if you created a folder in your google drive called 488, \n","   - and within that folder (488) you created another folder called Class03,\n","   - then you need to write the following: %cd /content/gdrive/MyDrive/488/Class03\n","   - DO NOT include the filename of the file you want to ultimately load inm your path!\n","5. make sure that the files you want to load are actually in that folder, that is, you uploaded them there (on your google drive)\n","6. to upload the files from Canvas to Google Drive, you first need to download them to your computer from Canvas and then upload them to Google Drive (unless you are automatically syncing your computer to the Google Cloud)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"jQWsDLwgOAg4"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/gdrive/MyDrive/488/Class03\n","488_2023_Class03_MissingData_20230117.ipynb\n","complaints-credit-debit-cards+fees-interest-2019-2020.csv\n","MoviesOnStreamingPlatforms_updated.csv\n"]}],"source":["# Let's assume you've created a folder called \"488\" on your Google drive for the course and a folder inside the \"488\" folder for class 3 called \"Class03\"\n","# You need to upload the files for this class (this .ipynb notbooks as well as the two CSV files) that you downloaded from Canvas to that folder.\n","\n","# Now that you Google Drive is connected, you can navigate to that folder with the %cd command as done below:\n","%cd /content/gdrive/MyDrive/488/Class03\n","\n","# To see waht files are in the folder, you can use a special shell command (!ls) to view the files in the home directory of the notebook environment\n","!ls "]},{"cell_type":"markdown","metadata":{"id":"faSJmiN2_vml"},"source":["# 1. Explore Missing Data\n","We will study missing data in a large data set about movies on streaming platforms. The data were scraped in 2020 from various platforms and merged with IMDb data (https://www.imdb.com/).    \n","\n","#### **Where to get the Data**  \n","\n","You can download the data set for this notebook from Kaggle:  \n","https://www.kaggle.com/ruchi798/movies-on-netflix-prime-video-hulu-and-disney"]},{"cell_type":"markdown","metadata":{"id":"0u5Rwyo2W-GY"},"source":["## 1.1 Read data into a DataFrame"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hKqb9d0kXGcZ"},"outputs":[],"source":["import pandas as pd\n","df = pd.read_csv(\"MoviesOnStreamingPlatforms_updated.csv\")\n","print(df.shape)\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"WE88Djp2XbqZ"},"source":["## 1.2 Check for Missing Data: Summary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2FRiM9VnXkFg"},"outputs":[],"source":["df.isna().sum()"]},{"cell_type":"markdown","metadata":{"id":"NEmgwuHQX0KD"},"source":["It looks like we have some missing data. Most of the missing data relate to \"***Rotten Tomatoes***\".  \n","\n","***Rotten Tomatoes*** score measures the percentage that are more positive than negative, and assigns an overall fresh or rotten rating to the movie. Scores of over 60 percent are considered fresh, and scores of 59 percent and under are rotten.   \n","https://www.vox.com/culture/2017/8/31/16107948/rotten-tomatoes-score-get-their-ratings-top-critics-certified-fresh-aggregate-mean"]},{"cell_type":"markdown","metadata":{"id":"jOHh6TDRYTcS"},"source":["## 1.3 Visually inspect Missing Data\n","Sometimes it is easier to see what is going on in your data when you visualize them.   \n","\n","The **Missingno library** offers a very nice way to visualize the distribution of NaN values.   \n","\n","Missingno is a Python library and compatible with Pandas. "]},{"cell_type":"markdown","metadata":{"id":"MEMIX6WPeKFp"},"source":["While missingno is available on CoLab, it may not be installed on your computer. If you want to use it, you may fist need to go to your Terminal and run: `pip install missingno`  \n","\n","See the github repository to learn more about missingno:  \n","https://github.com/ResidentMario/missingno"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vmVdwlVpZE-y"},"outputs":[],"source":["import missingno as msno\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LJMyUJGgZJrb"},"outputs":[],"source":["msno.matrix(df)"]},{"cell_type":"markdown","metadata":{"id":"6EIQfRbxeszA"},"source":["It looks like there is quite a lot of missing data (white areas in the Figure).  \n","\n","The sparkline at right summarizes the general shape of the data completeness and points out:\n","- the rows with the maximum \n","- the rows with the minimum     \n","\n","nullity in the dataset."]},{"cell_type":"markdown","metadata":{"id":"YDFN2a1oZTN0"},"source":["## 1.4 Systematically Missing?\n","Take a closer look - are missing values related to another? "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yzHKs2cpZsTc"},"outputs":[],"source":["msno.heatmap(df)"]},{"cell_type":"markdown","metadata":{"id":"uDMvu5cNcKu1"},"source":["The heatmap highlights positive correlation by the level level of darkness in blue (negative correlation would be red).   \n","\n","- Various levels of positive correlation exist between “Directors”, “Genres”, “Country”, “Language”, and “Runtime”\n","- The greatest positive correlation is between “Language” and “Country” (0.8)\n","- Apparently, there is a relationship between missing data.  \n","\n","***Ask yourself:***\n","1. What do you think the implications are? \n","2. What can we learn from these missing data?\n","3. Can we impute these missing data? \n","4. Should we impute them? Why or why not?\n","5. Should we drop observations with missing data? Why or why not?"]},{"cell_type":"markdown","metadata":{"id":"dtN-8ZrbAAa6"},"source":["# 2. Impute Missing Data in Time Series\n","\n","We will look at a time series of consumer complaints about credit and debit cards.  \n","\n","To illustrate the imputation of time series data:\n","1. We will create missing data in the series and \n","2. then try to fill them again with various imputation techniques.\n","\n","### Where to get the data: \n","You can download them directly from the *Consumer Financial Protection Bureau (CFPB):  \n","https://www.consumerfinance.gov/data-research/consumer-complaints/search/?chartType=line\u0026dateInterval=Week\u0026date_received_max=2020-12-31\u0026date_received_min=2019-01-01\u0026issue=Fees%20or%20interest\u0026lens=Overview\u0026product=Credit%20card%20or%20prepaid%20card\u0026searchField=all\u0026tab=Trends\n","\n","or  \n","\n","you can find the file ***complaints-credit-debit-cards+fees-interest-2019-2020.csv*** on CANVAS along with this notebook."]},{"cell_type":"markdown","metadata":{"id":"1AfihvKdEHjT"},"source":["## 2.1 Import Data and Prepare it"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VJcGG2XHBMaf"},"outputs":[],"source":["import pandas as pd\n","\n","# read csv file\n","df = pd.read_csv(\"complaints-credit-debit-cards+fees-interest-2019-2020.csv\")\n","\n","# show columns\n","display(df.columns.tolist())\n","\n","# show shape and the first 5 rows\n","print(df.shape)\n","df.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xU2RATR8BczX"},"outputs":[],"source":["# let's see which format the data is in\n","df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d2KqolNiCCpi"},"outputs":[],"source":["# Need to convert \"Date received\" to a pandas date time format\n","df['Date received']= pd.to_datetime(df['Date received'])\n","\n","# check that it worked\n","df.info()\n","display(df['Date received'].head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xEv-Jc7_C5AC"},"outputs":[],"source":["# Let's sort the data by date and check the range\n","df.sort_values(by=['Date received'], inplace=True)\n","display(df['Date received'].min())\n","display(df['Date received'].max())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1gF8Hi8eG-mv"},"outputs":[],"source":["# We  aggregate the number of complaints at the weekly level\n","weeklyOriginal=df[['Date received', 'Complaint ID']].groupby(pd.Grouper(key='Date received',freq='W')).count()\n","\n","# Let's rename the aggregated ID column to \"NumComplaints\"\n","weeklyOriginal = weeklyOriginal.rename(columns={'Complaint ID': 'NumComplaints'})\n","\n","# Take a look\n","weeklyOriginal.head()"]},{"cell_type":"markdown","metadata":{"id":"A5FnqnsyEU9c"},"source":["## 2.2 Remove Observations\n","\n","We are going to create some missing data for demonstration purposes. We will then try to recover them using imputation techniques."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W71VobJjIZwU"},"outputs":[],"source":["# Create a second dataframe\n","weekly=df[['Date received', 'Complaint ID']].groupby(pd.Grouper(key='Date received',freq='W')).count()\n","# Let's rename the aggregated ID column to \"NumComplaints\"\n","weekly = weeklyOriginal.rename(columns={'Complaint ID': 'NumComplaints'})\n","\n","# We set the complaint counts for select date ranges to NAN\n","import numpy as np\n","weekly[(weekly.index \u003e= pd.to_datetime(\"2019-01-14\")) \u0026 (weekly.index \u003c= pd.to_datetime(\"2019-02-10\"))] = np.nan\n","weekly[(weekly.index \u003e= pd.to_datetime(\"2019-04-08\")) \u0026 (weekly.index \u003c= pd.to_datetime(\"2019-04-28\"))] = np.nan\n","weekly[(weekly.index \u003e= pd.to_datetime(\"2019-06-24\")) \u0026 (weekly.index \u003c= pd.to_datetime(\"2019-08-25\"))] = np.nan\n","weekly[(weekly.index \u003e= pd.to_datetime(\"2019-09-23\")) \u0026 (weekly.index \u003c= pd.to_datetime(\"2019-10-27\"))] = np.nan\n","weekly[(weekly.index \u003e= pd.to_datetime(\"2020-01-20\")) \u0026 (weekly.index \u003c= pd.to_datetime(\"2020-02-24\"))] = np.nan\n","weekly[(weekly.index \u003e= pd.to_datetime(\"2020-09-17\")) \u0026 (weekly.index \u003c= pd.to_datetime(\"2020-11-22\"))] = np.nan\n","display(weekly.info())"]},{"cell_type":"markdown","metadata":{"id":"aWpxrK03L9WX"},"source":["## 2.3 Visualize our Time Series"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HXmcWYFHEe_e"},"outputs":[],"source":["# Missing Data\n","weeklyOriginal['NumComplaints'].plot(color='lightgrey', marker='o', linestyle='dotted', figsize=(30, 10))\n","weekly['NumComplaints'].plot(title='Weekly Complaints', marker='o', figsize=(30, 10))"]},{"cell_type":"markdown","metadata":{"id":"2bu1JSUcMW3w"},"source":["## 2.4 Impute Missing Data and Visualize"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fFbI3w1UMeFI"},"outputs":[],"source":["# Forward Fill\n","Forward = weekly.fillna(method='ffill')\n","weeklyOriginal['NumComplaints'].plot(color='lightgrey', marker='o', linestyle='dotted', figsize=(30, 10))\n","Forward['NumComplaints'].plot(color='red', marker='o', linestyle='dotted')\n","weekly['NumComplaints'].plot(title='Weekly Complaints', marker='o')\n","\n","# check correlation\n","display(weeklyOriginal['NumComplaints'].corr(Forward['NumComplaints']))\n","\n","# Correlation dataframe\n","display(pd.concat([weeklyOriginal, Forward], axis=1, keys=['weeklyOriginal', 'Forward']).corr().loc['Forward', 'weeklyOriginal'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-keWaU3iNcvR"},"outputs":[],"source":["# Back Fill\n","Backward = weekly.fillna(method='bfill')\n","weeklyOriginal['NumComplaints'].plot(color='lightgrey', marker='o', linestyle='dotted', figsize=(30, 10))\n","Backward['NumComplaints'].plot(color='red', marker='o', linestyle='dotted')\n","weekly['NumComplaints'].plot(title='Weekly Complaints', marker='o')\n","\n","# check correlation\n","display(weeklyOriginal['NumComplaints'].corr(Backward['NumComplaints']))\n","\n","# Correlation dataframe\n","display(pd.concat([weeklyOriginal, Backward], axis=1, keys=['weeklyOriginal', 'Backward']).corr().loc['Backward', 'weeklyOriginal'])"]},{"cell_type":"markdown","metadata":{"id":"Do7XzKdcz-m5"},"source":["### Notice that we repeat code in the above cells.   \n","\n","While it is easy and tempting to just copy/paste code, you will later want to make modifications, and your analysis will get out of sync.  Is there a way to avoid this?   \n","\n","**Yes, by using functions!**\n","\n","In the DataCamp assignments, we can learn how to write our own function. Let's refactor our code and organize it into a function.   "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1vVfmOXtnv1q"},"outputs":[],"source":["def impute_fill(impute_method, type):\n","  dt = weekly.fillna(method=impute_method)\n","  weeklyOriginal['NumComplaints'].plot(color='lightgrey', marker='o', linestyle='dotted', figsize=(30, 10))\n","  dt['NumComplaints'].plot(color='red', marker='o', linestyle='dotted')\n","  weekly['NumComplaints'].plot(title='Weekly Complaints', marker='o')\n","  display(weeklyOriginal['NumComplaints'].corr(dt['NumComplaints']))\n","  #CorrelationTable: display(pd.concat([weeklyOriginal, dt], axis=1, keys=['weeklyOriginal', type]).corr().loc[type, 'weeklyOriginal'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bmW3irsPpZ2A"},"outputs":[],"source":["impute_fill('ffill', 'Forward')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AWCVFu_fqH_s"},"outputs":[],"source":["impute_fill('bfill', 'Backward')"]},{"cell_type":"markdown","metadata":{"id":"CU1m_vhGFKVY"},"source":["## 2.5 Refactoring to do interpolation\n","\n","There are other imputation methods that interpolate rather than fill, but we can't just reuse the function we defined above.  This is because the function is doing more than one thing \n","* it assumes it is working on weekly data\n","* it does fill interpolation\n","* and then it displays results.  \n","\n","**When refactoring, make each function do one thing, and do that well.**\n","\n","Let's focus here on the plot. \n","-  What we do is show three time series layers.\n","-  From bottom to top, these are:\n","  -  ground truth, \n","  - the imputed, \n","  - and the given time series. \n","- We also display the correlation coefficient.  That description is both specific enough, and general enough, to be a function by itself. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MGwRw_3ONNKN"},"outputs":[],"source":["def plot3timelines(bottom, mid, top, mycolumn, mytitle):\n","  ''' This function plots time series from three dataframes with a common column '''\n","  bottom[mycolumn].plot(color='lightgrey', marker='o', linestyle='dotted', figsize=(30, 10))\n","  mid[mycolumn].plot(color='red', marker='o', linestyle='dotted')\n","  top[mycolumn].plot(title=mytitle, marker='o')\n","  display(bottom[mycolumn].corr(mid[mycolumn]))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o6fugIQfOJID"},"outputs":[],"source":["# Forward Fill \n","plot3timelines(weeklyOriginal, weekly.fillna(method='ffill'), weekly, 'NumComplaints', 'Weekly Complaints : Forward Fill')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PcWs0BZXN1c_"},"outputs":[],"source":["# Back Fill \n","plot3timelines(weeklyOriginal, weekly.fillna(method='bfill'), weekly, 'NumComplaints', 'Weekly Complaints : Backward Fill')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5PAuGGo6Nt5x"},"outputs":[],"source":["# Linear Interpolation\n","plot3timelines(weeklyOriginal, weekly.interpolate(method='linear'), weekly, 'NumComplaints', 'Weekly Complaints : Linear')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XJyj8LsPSIBf"},"outputs":[],"source":["# Quadratic Interpolation\n","plot3timelines(weeklyOriginal, weekly.interpolate(method='quadratic'), weekly, 'NumComplaints', 'Weekly Complaints : Quadratic')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U6c73lQKShEu"},"outputs":[],"source":["# Nearest Neighbor Interpolation\n","plot3timelines(weeklyOriginal, weekly.interpolate(method='nearest'), weekly, 'NumComplaints', 'Weekly Complaints : Nearest')"]},{"cell_type":"markdown","metadata":{"id":"CZcPhjcddKDe"},"source":["# 3. Tutorials and closing\n","Check out this nice tutorial on methods in pandas to handle missing data:  \n","https://towardsdatascience.com/handling-missing-values-with-pandas-b876bf6f008f\n","\n","The book, [Refactoring: improving the design of existing code](https://www.amazon.com/Refactoring-Improving-Existing-Addison-Wesley-Signature/dp/0134757599/ref=pd_lpo_14_t_0/144-9689246-2900853?_encoding=UTF8\u0026pd_rd_i=0134757599\u0026pd_rd_r=a09d615e-0fd1-4035-b56b-0d3e365a93af\u0026pd_rd_w=4n3e6\u0026pd_rd_wg=CDnNg\u0026pf_rd_p=16b28406-aa34-451d-8a2e-b3930ada000c\u0026pf_rd_r=EBHXKXGTW44N18AZXPCG\u0026psc=1\u0026refRID=EBHXKXGTW44N18AZXPCG), Fowler, is a classic.  Here are two summaries: https://github.com/HugoMatilla/Refactoring-Summary https://www.benlinders.com/2019/summary-of-refactoring-in-15-tweets/ "]},{"cell_type":"markdown","metadata":{"id":"3Q6pJ9czYI56"},"source":["## Never forget\n","\n","- being able to handle missing data is great\n","- ***BUT*** it does not mean that what you are doing with/to them is appropriate \n","- for the problem you are trying to solve"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LtEKQcvOHTEC"},"outputs":[],"source":[]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"}},"nbformat":4,"nbformat_minor":0}