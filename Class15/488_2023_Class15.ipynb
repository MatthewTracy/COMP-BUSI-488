{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"mkUYF0gHmDP_"},"source":["# **BUSI 488 / COMP 488 Data Science in the Business World**\n","## *Spring 2023*  \n","Daniel M. Ringel  \n","Kenan-Flagler Business School  \n","*The University of North Carolina at Chapel Hill*  \n","dmr@unc.edu\n","\n","## Class 15 - Ensemble Learning\n","\n","*March 2, 2023*  \n","Version 2.1\n","\n","![ensemble](https://music.unc.edu/files/2013/09/1484014_10151881768213591_277633653_o.jpg)\n","\n","**Ensemble learning** is the process by which multiple models, such as classifiers, are generated and combined to solve a particular problem better than single models could.    \n","\n","Ensemble learning is commonly used in machine learning to improve classification or regression models."]},{"cell_type":"markdown","metadata":{"id":"WC9eVEQhmCfI"},"source":["# Today's Agenda\n","\n","1. **Strengths and Limitations of CARTs**\n","2. **Wisdom of the Crowds in Machine Learning**\n","3. **Ensemble Learning with Bootstrap Aggregation (BAGGING)**\n","4. **Empirical Application of BAGGING: Predicting Loan Eligibility**\n","5. **Ensemble Learning with Random Forests: Pooling Experts**\n","6. **Adaptive Boosting: Can a Bunch of Fools beat the Experts?**\n","7. **Gradient Boosting with XGBoost**\n","\n","\n","## Prep-Check:\n","- Reviewed Notebook from Class 14"]},{"cell_type":"markdown","metadata":{"id":"_Zjhyk_WmCfP"},"source":["# 1. Strengths and Limitations of CARTs\n","*Classification and Regression Trees (CARTs)*\n","\n","## Strengths\n","- Clear Box\n","- Simple to understand\n","- Simple to interpret\n","- Easy to use\n","- Flexibility: ability to describe non-linear dependencies\n","- Preprocessing: no need to standardize or normalize features\n","\n","## Limitations\n","- Classification: can only produce orthogonal decision boundaries\n","- Sensitive to small variations in the training set\n","- High variance: unconstrained CARTs may ovefit the training set\n","- Solution: Wisdom of the Crowds!\n","\n","## Consequence:\n","- As far as accuracies of prediction go, Decision Trees are quite inaccurate. \n","- Even one mis-step in the choice of the next node, can lead you to a completely different end. \n","- Choosing the right branch instead of the left could lead you to the furthest end of the tree. \n","- You would be off by a huge margin!\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fBu2_ohEmCfP"},"source":["# 2. Wisdom of the Crowds in Machine Learning\n","\n","### *When you are uncertain about a decision:* \n","1. You might ask several people for their assessment ... \n","2. ... and then make a more informed decision.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"y_clRfnmmCfQ"},"source":["## It's April 2022: Will BitCoin go up?\n","\n","\n","![bitcoin](https://mapxp.app/BUSI488/Bitcoin-2022.png)\n","[source: CNBC](https://www.cnbc.com/2022/12/23/bitcoin-price-calls-in-2022-how-the-market-got-it-wrong.html#:~:text=Cryptocurrencies%20have%20been%20under%20pressure%20after%20the%20collapse%20of%20major%20exchange%20FTX.&text=2022%20marked%20the%20start%20of,of%20digital%20currencies%20crashing%20spectacularly)\n","\n","\n","- You have access to a dozen great analysts who have no prior knowledge about BitCoin. \n","- Each analyst has a low bias because they don’t have no prior assumptions.\n","- Each analyst can learn about BitCoin from a dataset of news reports. \n","- Each analyst has a high variance in their predicition when there is variation in the reports they receive.  \n","\n","***Having access to news reports seems ideal for the task!***\n","\n","### Problem: The reports are likely to contain noise in addition to real signals.\n","\n","**Because your analysts will base their predictions entirely on the data (they have complete flexibility how), they can be swayed by irrelevant information.**\n","\n","\n","### Consequence:\n","* The analysts might each come up with very differing predictions from the same set of reports. \n","* Each analyst might come up with dramatically different predicition when they are given a different set of reports.\n","\n","\n","### ***The solution is to not rely on any one individual, but pool the votes of multiple analysts.***\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9P-fR9M5mCfQ"},"source":["# 3. Ensemble Learning with Bootstrap Aggregation (BAGGING)\n","\n","- Uses a technique known as ***bootstrapping***.\n","- Reduces variance of individual models in an **ENSEMBLE** of the same algorithm\n","- One algorithm with different subsets of the training set.\n","\n","### ***Basic Idea:*** Create multiple Decision Trees based on bootstrapped samples and go with the majority vote of the trees!\n","\n","![BAGGING](https://mapxp.app/BUSI488/BAGGINGCART2.gif)"]},{"cell_type":"markdown","metadata":{"id":"pphHqObEmCfR"},"source":["# 4. Empirical Application of BAGGING: ***Predicting Eligibility for a Loan***\n","\n","[*Based on an Analytics Vidhya practice competition*](https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/)\n","\n","A Fintech wants to automate the loan eligibility process (in real time) based on the details that applicants provide in their online application. These details are:\n","\n","- Gender\n","- Marital Status\n","- Education\n","- Number of Dependents\n","- Income\n","- Loan Amount\n","- Credit History and others. \n","\n","To automate this process, the Fintech needs to assign applicants to either being eligible for the requested loan (amount), or not. You are provided with a training dataset in which the loan eligibility is marked. \n","The dataset contains the following variables:\n","\n","| Variable          \t| Description                     \t|\n","|-------------------\t|---------------------------------\t|\n","| Loan_ID           \t| Unique Loan ID                  \t|\n","| Gender            \t| Male/ Female                    \t|\n","| Married           \t| Applicant married (Y/N)         \t|\n","| Dependents        \t| Number of dependents            \t|\n","| Education         \t| Graduate/ Under Graduate        \t|\n","| Self_Employed     \t| Self employed (Y/N)             \t|\n","| ApplicantIncome   \t| Applicant income                \t|\n","| CoapplicantIncome \t| Coapplicant income              \t|\n","| LoanAmount        \t| Loan amount in thousands        \t|\n","| Loan_Amount_Term  \t| Term of loan in months          \t|\n","| Credit_History    \t| credit history meets guidelines \t|\n","| Property_Area     \t| Urban/ Semi Urban/ Rural        \t|\n","| Loan_Status       \t| Loan approved (Y/N)             \t|\n","\n","You can download the dataset ***loan_train.csv*** from CANVAS along with the notebook of this class."]},{"cell_type":"markdown","metadata":{"id":"kPRRFg_zZT1L"},"source":["## 4.1 Load the Data"]},{"cell_type":"code","metadata":{"id":"m1ObTrR3Tymw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677785377046,"user_tz":300,"elapsed":770,"user":{"displayName":"Matthew Tracy","userId":"03938494529490882151"}},"outputId":"db07d154-afe3-4005-f84f-8cc2f2076c72"},"source":["# 1. Connect out notebook with out Google Drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# 2. Navigate to the directory in which we saved the data file(s)\n","%cd /content/gdrive/MyDrive/488/Class15 "],"metadata":{"id":"z1HL6wveZWFA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677785377046,"user_tz":300,"elapsed":4,"user":{"displayName":"Matthew Tracy","userId":"03938494529490882151"}},"outputId":"5caea75c-f997-4292-8944-c45dec74215b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/488/Class15\n"]}]},{"cell_type":"code","source":["# 3. Check what files are in the current directory\n","!ls # special shell command to view the files in the home directory of the notebook environment (! command has no lasting effect)"],"metadata":{"id":"BIHe9nLB0Wap","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677785377196,"user_tz":300,"elapsed":153,"user":{"displayName":"Matthew Tracy","userId":"03938494529490882151"}},"outputId":"f43f06f4-15d9-49e9-c5fe-c933f04c1fbe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["488_2023_Class15.ipynb\tloan_train.csv\n"]}]},{"cell_type":"code","source":["# 4. Load data file into a pandas dataframe\n","import pandas as pd\n","data = pd.read_csv(\"loan_train.csv\") # parse to a Pandas DataFrame using pd.read_csv()\n","display(f\"Observations in Dataset: {len(data)}\")"],"metadata":{"id":"yh9EYIDYZSU5","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1677785377456,"user_tz":300,"elapsed":261,"user":{"displayName":"Matthew Tracy","userId":"03938494529490882151"}},"outputId":"215a01f5-2b09-48ab-e675-83819c567c01"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["'Observations in Dataset: 614'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"k2I70GMiBP1D"},"source":["## 4.2 Preprocess Data in one Step!\n","\n","We create functions that take care of all the preprocessing steps for us.  \n","\n","***What is the advantage of doing so?***"]},{"cell_type":"code","metadata":{"id":"RlhLktTZBZxS"},"source":["# 1a Import some libraries that you will need for this step\n","import numpy as np\n","import pandas as pd\n","from sklearn.impute import SimpleImputer\n","\n","# 2 Let's create two functions that help us with the preprocessing to make it faster and easier to execute\n","\n","# 2a OUTLIER DETECTION (we will call this function within our preprocessing function that we define next in 2b)\n","'This function can be used on any dataset to return a list of index values for the outliers (based on standard deviation)'\n","'Only appropriate for numerical features'\n","def get_outliers(data, columns, nsd=7):\n","    # we create an empty list\n","    outlier_idxs = []\n","    # Apply to columns\n","    for col in columns:\n","        elements = data[col]\n","        # we get the mean value for each column\n","        mean = elements.mean()\n","        # and the standard deviation of the column\n","        sd = elements.std()\n","        # we then get the index values of all values higher or lower than the mean +/- nsd standard deviations\n","        outliers_mask = data[(data[col] > mean + nsd*sd) | (data[col]  < mean  - nsd*sd)].index\n","        # and add those index values to our list\n","        outlier_idxs  += [x for x in outliers_mask]\n","    return list(set(outlier_idxs))\n","\n","# 2b Put all the PREPROCESSING from class 14 into a function\n","'Function that receives the loan eligibility raw data'\n","'as well as the imputation strategies for categorical and numerical features'\n","'and runs all preprocessing steps'\n","'Outputs a data object that contains features and response variable'\n","def prepro(df,numimp,catimp):\n","      #2b1 The first column (Loan_ID) is not informative to our task. So let's drop it!\n","      df=df.drop(['Loan_ID'],axis=1)\n","\n","      #2b2 Make your work easy: Automatically identify which features are numeric and which are categorical. \n","      # To do so, create two indices that hold the colum names of our data set that are numerical and categorical, respectively\n","      # Make sure that you do not include our response variable here, since this is just about features!\n","      numeric_features = df.select_dtypes(include=['int64', 'float64']).columns\n","      categorical_features = df.select_dtypes(include=['object', 'category']).drop(['Loan_Status'], axis=1).columns\n","\n","      #2b3 What about Credit_History? Should that not be a categorical variable?\n","      numeric_features=numeric_features.drop('Credit_History')\n","      categorical_features=categorical_features.insert(6, 'Credit_History') \n","    \n","      #2b4 Now typecast all variables that are categorical as type \"category\"\n","      for col in categorical_features.tolist():\n","              df[col] = df[col].astype('category')\n","\n","      #2b5 Don't forget to also typecast our response variable as category\n","      df['Loan_Status'] = df['Loan_Status'].astype('category')   \n","\n","      #2b6 Let's identify and remove outliers for the numerical variables using the function we created in 2a above\n","      outs = get_outliers(df, numeric_features)\n","      df = df.drop(outs, axis = 0)\n","\n","      #2b7a Now you need to impute missing numerical values. What imputation strategy will you choose?\n","      imputer = SimpleImputer(missing_values = np.nan, strategy = numimp)\n","      imputer = imputer.fit(df[numeric_features.tolist()])\n","      df[numeric_features.tolist()] = imputer.transform(df[numeric_features.tolist()])\n","\n","      #2b7b Now you need to impute missing categorical values. \n","      imputercat = SimpleImputer(missing_values = np.nan, strategy = catimp)\n","      imputercat = imputercat.fit(df[categorical_features.tolist()])\n","      df[categorical_features.tolist()] = imputercat.transform(df[categorical_features.tolist()])\n","      df['Credit_History'] = df['Credit_History'].astype('category')   # imputer set type back to numeric, so we correct it again\n","\n","      #2b8 Create a new dataframe X that includes only our feature variables\n","      feat = df.loc[:, df.columns != 'Loan_Status']\n","\n","      #2b9 Create a new dataframe y that includes only our \n","      lab = df.Loan_Status\n","\n","      #2b10 Now you need to one hot encode the categorical features to make them machine readable. \n","      feat = pd.get_dummies(feat)\n","\n","      #2b11 And re-code the response variable using a dictionary and replace\n","      repmap={\"Y\": 1, \"N\": 0}\n","      lab.replace(repmap, inplace=True)\n","\n","      #2b12 Feature engineer Household_Income_log (why log?) and add it to our features (X)\n","      feat['Household_Income_log']=np.log(feat.ApplicantIncome + feat.CoapplicantIncome)\n","\n","      #2b13 Feature engineer EMI and add it to our features (X) --> See class 14 for details on intution and formal euqations\n","      r = 0.06/12\n","      feat['EMI']=np.ceil(feat['LoanAmount']*1000*r*((1+r)**feat['Loan_Amount_Term'])/((1+r)**feat['Loan_Amount_Term']-1)*100)/100\n","\n","      #2b14 Append our two new features to our numerical features\n","      numeric_features=numeric_features.insert(len(numeric_features),'EMI') \n","      numeric_features=numeric_features.insert(len(numeric_features),'Household_Income_log') \n","      \n","      #2b15 Return the features (feat) and labels (lab)\n","      return feat, lab, numeric_features"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### ***Let's apply our functions to the data!***"],"metadata":{"id":"shnw1mttaF9W"}},{"cell_type":"code","metadata":{"id":"XiqiXdpVHaq6","colab":{"base_uri":"https://localhost:8080/","height":500},"executionInfo":{"status":"ok","timestamp":1677785378753,"user_tz":300,"elapsed":192,"user":{"displayName":"Matthew Tracy","userId":"03938494529490882151"}},"outputId":"f4b5227d-1275-4cb5-f3cb-d719631e0d0c"},"source":["# 3. Call PREPROCESSING function and pass it the dataframe as well as the imputation strategies for (1) numerical and (2) categorical features\n","data = prepro(data,'median','most_frequent') \n","\n","# 4. Separate features (X) and labels (y) from data object\n","X=data[0]\n","y=data[1]\n","\n","# 5. Get numerical features\n","numeric_features = data[2]\n","\n","# 6. Verify that it worked\n","display(X.head())\n","display(y.head())\n","display(numeric_features)\n","display(f\"Observations after Preprocessing: {len(X)}\")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n","0           5849.0                0.0       127.0             360.0   \n","1           4583.0             1508.0       128.0             360.0   \n","2           3000.0                0.0        66.0             360.0   \n","3           2583.0             2358.0       120.0             360.0   \n","4           6000.0                0.0       141.0             360.0   \n","\n","   Gender_Female  Gender_Male  Married_No  Married_Yes  Dependents_0  \\\n","0              0            1           1            0             1   \n","1              0            1           0            1             0   \n","2              0            1           0            1             1   \n","3              0            1           0            1             1   \n","4              0            1           1            0             1   \n","\n","   Dependents_1  ...  Education_Not Graduate  Self_Employed_No  \\\n","0             0  ...                       0                 1   \n","1             1  ...                       0                 1   \n","2             0  ...                       0                 0   \n","3             0  ...                       1                 1   \n","4             0  ...                       0                 1   \n","\n","   Self_Employed_Yes  Credit_History_0.0  Credit_History_1.0  \\\n","0                  0                   0                   1   \n","1                  0                   0                   1   \n","2                  1                   0                   1   \n","3                  0                   0                   1   \n","4                  0                   0                   1   \n","\n","   Property_Area_Rural  Property_Area_Semiurban  Property_Area_Urban  \\\n","0                    0                        0                    1   \n","1                    1                        0                    0   \n","2                    0                        0                    1   \n","3                    0                        0                    1   \n","4                    0                        0                    1   \n","\n","   Household_Income_log     EMI  \n","0              8.674026  761.43  \n","1              8.714568  767.43  \n","2              8.006368  395.71  \n","3              8.505323  719.47  \n","4              8.699515  845.37  \n","\n","[5 rows x 23 columns]"],"text/html":["\n","  <div id=\"df-92294100-f4fd-4dff-80bb-f905bcefcf7f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ApplicantIncome</th>\n","      <th>CoapplicantIncome</th>\n","      <th>LoanAmount</th>\n","      <th>Loan_Amount_Term</th>\n","      <th>Gender_Female</th>\n","      <th>Gender_Male</th>\n","      <th>Married_No</th>\n","      <th>Married_Yes</th>\n","      <th>Dependents_0</th>\n","      <th>Dependents_1</th>\n","      <th>...</th>\n","      <th>Education_Not Graduate</th>\n","      <th>Self_Employed_No</th>\n","      <th>Self_Employed_Yes</th>\n","      <th>Credit_History_0.0</th>\n","      <th>Credit_History_1.0</th>\n","      <th>Property_Area_Rural</th>\n","      <th>Property_Area_Semiurban</th>\n","      <th>Property_Area_Urban</th>\n","      <th>Household_Income_log</th>\n","      <th>EMI</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5849.0</td>\n","      <td>0.0</td>\n","      <td>127.0</td>\n","      <td>360.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>8.674026</td>\n","      <td>761.43</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4583.0</td>\n","      <td>1508.0</td>\n","      <td>128.0</td>\n","      <td>360.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>8.714568</td>\n","      <td>767.43</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3000.0</td>\n","      <td>0.0</td>\n","      <td>66.0</td>\n","      <td>360.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>8.006368</td>\n","      <td>395.71</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2583.0</td>\n","      <td>2358.0</td>\n","      <td>120.0</td>\n","      <td>360.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>8.505323</td>\n","      <td>719.47</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>6000.0</td>\n","      <td>0.0</td>\n","      <td>141.0</td>\n","      <td>360.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>8.699515</td>\n","      <td>845.37</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 23 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-92294100-f4fd-4dff-80bb-f905bcefcf7f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-92294100-f4fd-4dff-80bb-f905bcefcf7f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-92294100-f4fd-4dff-80bb-f905bcefcf7f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["0    1\n","1    0\n","2    1\n","3    1\n","4    1\n","Name: Loan_Status, dtype: int64"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Index(['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount',\n","       'Loan_Amount_Term', 'EMI', 'Household_Income_log'],\n","      dtype='object')"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["'Observations after Preprocessing: 609'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"9UPAqcWWmCfW"},"source":["## 4.3 Baseline Predicition with a Classification Tree\n","\n","### Let's take a look at how well we can predict customer eligibility using a Classification Tree"]},{"cell_type":"code","metadata":{"id":"WPdAjxoymCfX"},"source":["# 1. Import DecisionTreeClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","\n","# 2. Import train_test_split\n","from sklearn.model_selection import train_test_split\n","\n","# 3. Import accuracy_score\n","from sklearn.metrics import accuracy_score\n","\n","# 4. Split dataset into 70% train, 30% test\n","X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n","\n","# 5. Instantiate dt, set 'criterion' to 'gini'\n","dt = DecisionTreeClassifier(criterion='gini', random_state=1)\n","\n","# 6. Fit dt to the training set\n","dt.fit(X_train,y_train)\n","\n","# 7. Predict test-set labels\n","y_pred_dt = dt.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VMA_3wsCmCfX"},"source":["### So how well does our Classification Tree do?"]},{"cell_type":"code","metadata":{"id":"GV76CAZ2mCfX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677785378754,"user_tz":300,"elapsed":6,"user":{"displayName":"Matthew Tracy","userId":"03938494529490882151"}},"outputId":"9fb4b976-2ce1-4d83-e55d-e00958bb44ac"},"source":["# 8. Output the accuracy of our prediction\n","from sklearn.metrics import accuracy_score\n","print(f\"Accuracy of Decision Tree Classifier is {round(accuracy_score(y_test, y_pred_dt)*100,2)}%\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of Decision Tree Classifier is 74.32%\n"]}]},{"cell_type":"markdown","metadata":{"id":"ivEp2CzJmCfY"},"source":["## 4.4 Improved Prediction with BAGGING\n","\n","![Forest](https://mapxp.app/BUSI488/paperbags3.jpg)"]},{"cell_type":"code","metadata":{"id":"-ub835KemCfY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677785592966,"user_tz":300,"elapsed":368,"user":{"displayName":"Matthew Tracy","userId":"03938494529490882151"}},"outputId":"c8e75d83-6516-482f-c17b-634f84e4fba8"},"source":["# 1. Import models and utility functions\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","# 2. Split data into 70% train and 30% test\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n","\n","# 3. Instantiate a classification-tree 'dt'\n","dt = DecisionTreeClassifier(criterion='gini')\n","\n","# 4. Instantiate a BaggingClassifier 'bc'\n","bc = BaggingClassifier(estimator=dt, n_estimators=25, n_jobs=-1,random_state=42)\n","\n","# 5. Fit 'bc' to the training set\n","bc.fit(X_train, y_train)\n","\n","# 6. Predict test set labels\n","y_pred_bag = bc.predict(X_test)\n","\n","# 7. Output the accuracy of our prediction\n","print(f\"Accuracy of Bagging Classifier is {round(accuracy_score(y_test, y_pred_bag)*100,2)}%\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of Bagging Classifier is 79.23%\n"]}]},{"cell_type":"markdown","metadata":{"id":"64DMJm22mCfa"},"source":["# 5. Ensemble Learning with Random Forests: Pooling Experts\n","![Forest](https://mapxp.app/BUSI488/trees.jpg)"]},{"cell_type":"markdown","metadata":{"id":"dHLKQauomCfa"},"source":["### Building a Random Forest\n","\n","There are 4 steps involved in building a random forest:\n","\n","    1. Create a ‘bootstrapped dataset’ from the original data.\n","    2. Create a decision tree using these bootstrapped data: sample features\n","    3. Rinse and Repeat 1 and 2 to create more decision trees.\n","    4. Take majority vote (Classification) or average (Regression)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vFYdwCIWmCfb"},"source":["### Feature Randomness  \n","\n","In a normal decision tree, when it is time to split a node, we consider every possible feature and pick the one that produces the most separation between the observations in the left node vs. those in the right node.   \n","\n","In contrast, each tree in a random forest can pick only from a random subset of features.  \n","\n","**This forces even more variation amongst the trees in the model and ultimately results in lower correlation across trees and more diversification.**"]},{"cell_type":"markdown","metadata":{"id":"4ujiC1q4mCfb"},"source":["A large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models.\n"]},{"cell_type":"markdown","metadata":{"id":"tvp5oZH-mCfb"},"source":["![BAGGING](https://mapxp.app/BUSI488/RandomForest2.gif)"]},{"cell_type":"markdown","metadata":{"id":"-9EDf8IamCfb"},"source":["## 5.1 Improved Prediction with a Random Forest"]},{"cell_type":"code","metadata":{"id":"KBQ49VqDmCfb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677785959649,"user_tz":300,"elapsed":302,"user":{"displayName":"Matthew Tracy","userId":"03938494529490882151"}},"outputId":"68aaecf0-1672-465a-edda-0bcc57092c7d"},"source":["# 1. Basic imports\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","\n","# 2. Split data into 70% train and 30% test\n","X_train, X_test, y_train, y_test = train_test_split(X, y,\n","test_size=0.3,\n","stratify=y,\n","random_state=42)\n","\n","# 3. Instantiate a random forests classifier\n","rf = RandomForestClassifier(n_estimators=25, bootstrap = True, max_features = 'sqrt', min_samples_leaf = 5, criterion='gini', random_state=42)\n","\n","# 4. Fit 'rf' to the training set\n","rf.fit(X_train, y_train)\n","\n","# 5. Predict the test set labels 'y_pred'\n","y_pred_rf = rf.predict(X_test)\n","\n","# 6. Output the accuracy of our prediction\n","print(f\"Accuracy of Random Forest Classifier is {round(accuracy_score(y_test, y_pred_rf)*100,2)}%\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of Random Forest Classifier is 83.06%\n"]}]},{"cell_type":"markdown","metadata":{"id":"8q06JdVvvkve"},"source":["# 6. Adaptive Boosting: Can a Bunch of Fools beat the Experts?\n","![Idiots](https://mapxp.app/BUSI488/3stooges.jpg)\n","___ \n","#### *I'd rather rely on a hundred fools that learn from each other's mistakes, than a hundred opinionated experts.*\n","*Daniel M. Ringel*   \n","___ \n","\n","**We saw that many experts can improve prediction. We call it *Random Forests*.**  \n","\n","Let's see if a bunch of fools can do better than our experts in the Random Forest!\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YXVPnfwN2LYV"},"source":["## 6.1 Boosting\n","\n","**Boosting:** Ensemble method combining several weak learners to form a strong learner.  \n","\n","**Weak learner:** Model doing slightly better than random guessing.  \n","\n","**Example of weak learner:** Decision stump (CART whose maximum depth is 1).  \n","\n","\n","**Basic Idea:**\n","- Train an ensemble of predictors sequentially.\n","- Each predictor tries to correct its predecessor.\n","- Most popular boosting methods:\n","    * AdaBoost\n","    * Gradient Boosting"]},{"cell_type":"markdown","metadata":{"id":"_WO2K_e_2UWH"},"source":["## 6.2 AdaBoost\n","\n","- Stands for **Adaptive Boosting**.\n","- Each predictor pays more attention to the instances wrongly predicted by its predecessor.\n","- Achieved by changing the weights of training instances.\n","- Each predictor is assigned a coefficient $\\alpha$.\n","- $\\alpha$ (alpha) depends on the predictor's training error.\n","- $\\eta$ (eta) is the learning rate (shrinkage of $\\alpha$)\n","    * small $\\eta$ means the models trains slower (and sometimes better), but you need more estimators\n","\n","![AdaBoostProcess](https://mapxp.app/BUSI488/AdaBoostProcess.jpg)\n","\n","\n","### AdaBoost used both for Classification and Regression\n","- Classification:\n","    * Weighted majority voting.\n","    * In sklearn: AdaBoostClassifier\n","- Regression:\n","    * Weighted average.\n","    * In sklearn: AdaBoostRegressor\n","    \n","    \n","*Source: DataCamp*"]},{"cell_type":"markdown","metadata":{"id":"rZEMlrZZtSX2"},"source":["## 6.3 Random Forest vs. AdaBoost (aka Experts vs. Fools)\n","\n","#### Basic Idea of a Random Forest\n","- Each tree is a full-size tree\n","- Each tree can look different in terms of depth and splits\n","- Each tree can use multiple features to make a decision\n","- Each tree has equal weight in voting\n","- Each tree is made independently of the others\n","- Most trees have different samples to operate on (because of bootstrapping)\n","\n","\n","#### Basic Idea of AdaBoost for Decision Trees\n","- Usually uses stumps (root node with two leaves)\n","- Forest of stumps\n","- Stumps can only use one variable to make a decision (split)\n","- Stumps are not good at making accurate classifications = weak learners\n","- Some stumps have a greater say (i.e., significance) in the voting at the end than others (weighted by their accuracy)\n","- Stump order matters! Stumps are created in sequence, whereby the next stump implicitly takes the errors made by its predecessor into account for its own classification.\n"]},{"cell_type":"markdown","metadata":{"id":"rr8A6lnD2veW"},"source":["## 6.4 Steps in the AdaBoost Algorithm\n","1. Initialize a weight ($w_r$) for each record that indicates how important it is for the classification.  \n","   At the start, all records get the same weight ($1/R$), which makes them equally important.    \n","    \n","   \n","2. Make the best split: Determining which feature to split on based on the information gain (see class on decision trees for details)  \n","\n","\n","3. Determine the total error of the stump: Sum of the weights ($w_r$) of the incorrectly classified records.  \n","\n","\n","4. Use the error of the classifier (i.e., the share of misclassified records) to determine how much say (i.e., vote) the particular stump has in the final classification, which is captured by $\\alpha$ (also referred to as significance).  \n","\n","\n","5. Update (i.e., increase or decrease) the weights ($w_r$) for the records based on $\\alpha$   \n","   and normalize all weights so that they sum to 1 (i.e., divide ***all*** records' weights by their sum).  \n","   \n","   \n","6. Create a new dataset of records:\n","    - of the same shape as the original dataset (i.e., the same number of rows and columns) \n","    - based on the normalized sample weights\n","    - by drawing $R$ records (with replacement)\n","    - where for each of $R$ draws you generate a random number between 0 and 1\n","    - and add the record to the new dataset whose cumulative weight (i.e., the sum of weights of all records \"above\" it in the table)\n","    - the randomly drawn number falls into.\n","\n","    \n","7. Repeat steps 2-6. "]},{"cell_type":"markdown","metadata":{"id":"rwGaxZ8p27O3"},"source":["## 6.5 Calculating Alpha\n","$\\alpha_t= \\frac{1}{2}ln\\left ( \\frac{1-\\epsilon_t}{\\epsilon_t} \\right )= \\;Significance\\; = \\frac{1}{2}ln\\left ( \\frac{1-Total\\; Error_t}{Total\\;  Error_t} \\right )$\n","\n","***Let's visualize Alpha for different error rates:***"]},{"cell_type":"code","metadata":{"id":"KAwUpe8A3Fjq","colab":{"base_uri":"https://localhost:8080/","height":422},"executionInfo":{"status":"ok","timestamp":1677785380358,"user_tz":300,"elapsed":471,"user":{"displayName":"Matthew Tracy","userId":"03938494529490882151"}},"outputId":"9a5d12ad-2476-4183-b6a6-89659aa15d66"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","plt.rcParams['font.size'] = 18  # makes label size bigger\n","plt.figure(figsize=(6,6))\n","t = np.arange(0.01, 0.99, 0.001)\n","s = (1/2)*np.log((1-t)/t)\n","line, = plt.plot(t, s, lw=2)\n","plt.ylim(-2.5, 2.5)\n","plt.xlabel('Error Rate',fontsize = 14)\n","plt.ylabel('Alpha',fontsize = 14)\n","plt.title('Significance in AdaBoost')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x432 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"zIdZixFN3LEf"},"source":["**The above graph shows the intuition behind alpha in the final vote of the AdaBoost classifier:**\n","\n","1. The classifier weight grows exponentially as the error approaches 0.   \n","    *Better classifiers are given exponentially more weight.*  \n","    \n","\n","2. The classifier weight is zero if the error rate is 0.5.   \n","     *A classifier with 50% accuracy is no better than random guessing, so we ignore it.*  \n","     \n","\n","3. The classifier weight grows exponentially negative as the error approaches 1.   \n","     *We give a negative weight to classifiers with worse than 50% accuracy.*   \n","     *Whatever that classifier says, do the opposite!* "]},{"cell_type":"markdown","metadata":{"id":"6Fk3dolW3L0m"},"source":["## 6.6 Updating the Weights\n","We now update the weight for each record based on how well the current stump did overall and specifically for each record. \n","\n","**Incorrectly classified records**\n","\n","$w_{r,t+1} = w_{r,t} \\times e^{\\alpha_t} = New \\;Record \\;Weight \\;= \\;Record \\;Weight \\;\\times\\; e^{Significance}$\n","\n","*There is nothing fancy going on here!*  \n","*All we do is raise $e$ to the power of the significance $\\alpha_t$*  \n","*so that the new record weight grows exponentially*  \n","*(which increases its chance of being drawn for the sample of the next stump).*  \n","\n","\n","**Correctly classified records**  \n","Finally, look at the records that the tree classified correctly,  \n","and decrease their associated weights using the following formula:  \n","\n","$w_{r,t+1} = w_{r,t} \\times e^{- \\alpha_t} = New \\;Record \\;Weight \\;= \\;Record \\;Weight \\;\\times \\;e^{- Significance}$\n","\n","**Normalize Weights**  \n","Finally, we normalize all weights (for all records) by dividing the new weights by the sum of new weights.   \n","\n","$\\frac{w_{r,t+1}}{\\sum_{r\\in R}w_{r,t+1}}$\n"]},{"cell_type":"markdown","metadata":{"id":"qjparEGe3R7n"},"source":["## 6.7 Constructing the New  Dataset\n","\n","![AdaBoost1200](https://mapxp.app/BUSI488/AdaBoost1200.jpg)"]},{"cell_type":"markdown","metadata":{"id":"WuDsnr2i3ZDY"},"source":["## 6.8 Desirable Outcomes! \n","\n","**Records which the previous stump incorrectly classified**\n","- should be associated with larger weights ...   \n","  ... so that they are more likely to be in the sample of the next stump   \n","  \n","  \n","- than the records that the current stump classified correctly ...   \n","  ... which will receive smaller weights and are thus less likely to be drawn in the next sample    \n","  \n","\n","\n","**Since the records that were incorrectly classified** have higher weights in relation to the others,   \n","      ... the likelihood that the random number falls under their slice of the distribution is greater\n","   \n","**Consequence:** \n","-  The new dataset will have a tendency to contain multiple copies of the records that were misclassified by the previous tree\n","- When we go back to the step where we evaluate the predictions made by each decision tree,   \n","  the one with the highest score will have correctly classified the records the previous tree misclassified.\n"]},{"cell_type":"markdown","metadata":{"id":"ZuVcb2Po3fg2"},"source":["## 6.9 Let's have some Fools predict loan eligibility! (AdaBoost) \n","\n","![chainsaw](https://mapxp.app/BUSI488/chainsaw.jpg)"]},{"cell_type":"code","metadata":{"id":"ojDj-Zhf3zz3"},"source":["# Import models and utility functions\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","import IPython\n","from sklearn.model_selection import train_test_split\n","\n","# Split data into 70% train and 30% test\n","#Already done: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n","\n","# # Instantiate an \"fool\" classification-tree 'fool'\n","fool = DecisionTreeClassifier(max_depth=1, criterion='gini', min_samples_leaf = 5, splitter = \"random\")\n","\n","# Instantiate an AdaBoost classifier 'adab_clf'\n","adb_clf = AdaBoostClassifier(estimator=fool, n_estimators=25, learning_rate=.3, random_state=42)\n","\n","# Fit 'adb_clf' to the training set\n","adb_clf.fit(X_train, y_train)\n","\n","# Predict the test data\n","y_pred_ada = adb_clf.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eUToimar6cda"},"source":["### ***So how about them fools?***\n","Can they do better? Let's check!"]},{"cell_type":"code","metadata":{"id":"xpAdVyok5DS5","colab":{"base_uri":"https://localhost:8080/","height":148},"executionInfo":{"status":"ok","timestamp":1677785380358,"user_tz":300,"elapsed":6,"user":{"displayName":"Matthew Tracy","userId":"03938494529490882151"}},"outputId":"1dfab217-c068-4681-d6cf-7fd6ca406099"},"source":["from sklearn.metrics import accuracy_score\n","print(f'Decision Tree Accuracy: {round(accuracy_score(y_test, y_pred_dt)*100,2)}%')\n","print(f'Bagging Accuracy: {round(accuracy_score(y_test, y_pred_bag)*100,2)}%')\n","print(f'Random Forest Accuracy: {round(accuracy_score(y_test, y_pred_rf)*100,2)}%')\n","print(f'Adaptive Boosting Accuracy: {round(accuracy_score(y_test, y_pred_ada)*100,2)}%')\n","if accuracy_score(y_test, y_pred_ada) > accuracy_score(y_test, y_pred_rf): IPython.display.display(IPython.display.Audio(url=\"https://mapxp.app/BUSI488/foolswin.wav\", autoplay=True))  "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Decision Tree Accuracy: 74.32%\n","Bagging Accuracy: 79.23%\n","Random Forest Accuracy: 83.06%\n","Adaptive Boosting Accuracy: 84.7%\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.lib.display.Audio object>"],"text/html":["\n","                <audio  controls=\"controls\" autoplay=\"autoplay\">\n","                    <source src=\"https://mapxp.app/BUSI488/foolswin.wav\" type=\"audio/x-wav\" />\n","                    Your browser does not support the audio element.\n","                </audio>\n","              "]},"metadata":{}}]},{"cell_type":"markdown","source":["# 7. XGBoost: Extreme Gradient Boosting\n","\n","***XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable.***\n","\n","- Scalable, distributed gradient-boosted decision tree (GBDT) machine learning library.\n","- Uses ensemble learning, that is, an ensemble of decision trees.\n","- Implements machine learning algorithms under the Gradient Boosting framework. \n","- Provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. \n","- The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.\n","\n","**Gradient boosting** is \n","- An extension of boosting where the process of additively generating weak models is formalized as a gradient descent algorithm over an objective function. \n","- Gradient boosting sets targeted outcomes for the next model in an effort to minimize errors. \n","- Targeted outcomes for each case are based on the gradient of the error (hence the name gradient boosting) with respect to the prediction.\n","\n","***Random forest “bagging” minimizes the variance and overfitting, whereas GBDT “boosting” minimizes the bias and underfitting.***\n","\n","*XGBoost is the leading machine learning library for regression, classification, and ranking problems:* \n","\n","\n","\n","[Read more about XGBoost](https://xgboost.readthedocs.io/en/stable/index.html)"],"metadata":{"id":"4BhrMiNtWahv"}},{"cell_type":"code","source":["# Import XGBoost\n","import xgboost as xgboost\n","\n","# Instantiate Classifier\n","xgb = xgboost.XGBClassifier(n_estimators=9, subsample=.8,colsample_bytree=.2, eta=.3, seed=42)\n","\n","# Fit Classifier to training data\n","xgb.fit(X_train, y_train)\n","\n","# Predict test data with trained classifier\n","y_pred_xg = xgb.predict(X_test)\n","\n","# Evaluate performance of test predictions\n","print(f'Extreme Gradient Boosting Accuracy: {round(accuracy_score(y_test, y_pred_xg)*100,2)}%')"],"metadata":{"id":"U25LS3C__AYH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677787956388,"user_tz":300,"elapsed":89,"user":{"displayName":"Matthew Tracy","userId":"03938494529490882151"}},"outputId":"cb36693c-4192-4772-f400-85bf24054a53"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Extreme Gradient Boosting Accuracy: 85.25%\n"]}]},{"cell_type":"code","source":["# Import necessary libraries\n","from sklearn.model_selection import GridSearchCV\n","import xgboost as xgb\n","\n","# Define the hyperparameter grid\n","param_grid = {\n","    'n_estimators': [5, 7, 8,9, 10, 15],\n","    'subsample': [0.5, 0.8],\n","    'colsample_bytree': [0.2, 0.5],\n","    'eta': [0.1, 0.3, 0.5],\n","    'seed': [42]\n","}\n","\n","# Instantiate the XGBoost classifier\n","xgb = xgb.XGBClassifier()\n","\n","# Create a GridSearchCV object\n","grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=5)\n","\n","# Fit the grid search object to the training data\n","grid_search.fit(X_train, y_train)\n","\n","# Get the best hyperparameters and the corresponding accuracy score\n","best_params = grid_search.best_params_\n","best_score = grid_search.best_score_\n","\n","# Print the best hyperparameters and the corresponding accuracy score\n","print(\"Best hyperparameters: \", best_params)\n","print(\"Best accuracy score: \", best_score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o8lEn3GK89qL","executionInfo":{"status":"ok","timestamp":1677788006041,"user_tz":300,"elapsed":9137,"user":{"displayName":"Matthew Tracy","userId":"03938494529490882151"}},"outputId":"92fe87aa-71c1-4e9f-c82c-951307860115"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Best hyperparameters:  {'colsample_bytree': 0.2, 'eta': 0.1, 'n_estimators': 7, 'seed': 42, 'subsample': 0.8}\n","Best accuracy score:  0.7910533515731875\n"]}]},{"cell_type":"markdown","metadata":{"id":"nDOnv94I6R5G"},"source":["# **Looking Ahead:**  \n","\n","####**Next Class:** Tuesday, March 7, 2023 \n","\n","#### ***Model Performance and Generalization Errors***\n","\n","#### Reading: None\n","\n"]},{"cell_type":"markdown","metadata":{"id":"F_LFp6mPmCff"},"source":["This notebooks is in part based on the following excellent tutorials:\n","\n","https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76  \n","https://towardsdatascience.com/understanding-random-forest-58381e0602d2  \n","https://medium.com/x8-the-ai-community/building-intuition-for-random-forests-76d36fa28c5e  \n","https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/Random%20Forest%20Tutorial.ipynb  "]}]}